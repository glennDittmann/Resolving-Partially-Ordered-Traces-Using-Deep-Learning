--- Questions for BA---


2. should we only consider encoding the actually appearing uncertain sequences? this greatly reduces feature vector length, for instance from ~340,000 to 86 
 even for smallest log (smallest number of single events = 9) the encoding with all possible sequences for up to k=4 explodes in colabe ram when using the second encoding (we have 7380 possible sequences for 9 events)
 
   
5. how deep to go in preliminaries


6. how many pages

-----
since encoding with really all possible resolutins, i.e. building all possible combinations from the activities in the activity universe for up to k 
where k was 4, exploded the RAM in encoding either using first or second approach, i changed the encoding as follows

1. look up which kind of event sets appear in the log
2. build all possible resolutions for each event set, i.e. {a}->a and {ab}->ab, ba

3.1 first encoding takes the above list and uses one hot encoding, where the solution y for an event set x is the order the events appeared in the log

3.2 second encoding takes each subset for the event sets that appear and adds it to input encoding
		then an event set x encodes for itself as before but also for all subsets, e.g. {a,b} encodes for {a},{b},{a,b} and thus vector has three 1's
	and for each possible resolution we also build its subsequences and at it to target encoding, e.g.
		the sequence abc encodes for a,b,c,ab,bc,abc and thus the vector has six 1's
		
with this we greatly reduce the vector space and don't explode RAM

however with more computation power we could always look for every possible subset and subsequence

ALSO: check the distribution of uncertain event sets!
