{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "encoding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "badge": true,
        "repo_name": "glennDittmann/bachelor_thesis",
        "branch": "master",
        "nb_path": "notebooks/encoding.ipynb",
        "comment": "This badge cell was added by colab-badge-action"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glennDittmann/bachelor_thesis/blob/master/notebooks/encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmQiKOVab1yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# for encoding a log we need:\n",
        "# the vocab, i.e. the activity space -> a python list of the events\n",
        "# the number of oov_buckets, lets say 2 for now\n",
        "# creating a lookup table with the indices for each possible activity\n",
        "# functions that encode a trace, i.e. a sequence of events into an one_hot vector, embedding respectively -> getting traces as python lists as input\n",
        "\n",
        "class log_encoder():\n",
        "\n",
        "    def __init__(self, activity_list, num_oov_bucktes, embedding_dim=None):\n",
        "        self.activities = activity_list\n",
        "        self.num_oov_buckets = num_oov_bucktes\n",
        "\n",
        "        self.indices = tf.range(len(self.activities), dtype=tf.int64)\n",
        "        self.table_init = tf.lookup.KeyValueTensorInitializer(self.activities, self.indices)\n",
        "        self.table = tf.lookup.StaticVocabularyTable(self.table_init, self.num_oov_buckets)\n",
        "\n",
        "        #only needed for embeddings\n",
        "        if embedding_dim:\n",
        "            embed_init = tf.random.uniform([len(self.activities)+self.num_oov_buckets, embedding_dim])\n",
        "            self.embedding_matrix = tf.Variable(embed_init)\n",
        "\n",
        "    def lookup_indices(self, trace):\n",
        "        categories = tf.constant(trace)\n",
        "        cat_indices = self.table.lookup(categories)\n",
        "        return cat_indices\n",
        "\n",
        "    def one_hot_encode_trace(self, trace):\n",
        "        cat_indices = self.lookup_indices(trace)\n",
        "        cat_one_hot = tf.one_hot(cat_indices, depth=len(self.activities) + self.num_oov_buckets)\n",
        "        return cat_one_hot\n",
        "\n",
        "    def one_hot_encode_log(self, log):\n",
        "        #log is not in the format as when loaded with pm4py, but rather a list(=log) of lists(=traces) containing the activities as strings\n",
        "        encoded_inputs = []\n",
        "        for trace in log:\n",
        "            encoded_inputs.append(self.one_hot_encode_trace(trace))\n",
        "        \n",
        "        return encoded_inputs\n",
        "\n",
        "    def embed_encode_trace(self, trace):\n",
        "        cat_indices = self.lookup_indices(trace)\n",
        "        cat_embed = tf.nn.embedding_lookup(self.embedding_matrix, cat_indices)\n",
        "        return cat_embed\n",
        "\n",
        "    def embed_encode_log(self, log):\n",
        "        #log is not in the format as when loaded with pm4py, but rather a list(=log) of lists(=traces) containing the activities as strings\n",
        "        encoded_inputs = []\n",
        "        for trace in log:\n",
        "            encoded_inputs.append(self.embed_encode_trace(trace))\n",
        "        \n",
        "        return encoded_inputs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}